{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99b06d4e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " <center> <h1> <b> Pattern Recognition and Machine Learning (EE5610 - EE2802 - AI2000 - AI5000) </b> </h1> </center>\n",
    "\n",
    "<b> Programming Assignment - 04 : Neural Networks </b>\n",
    "\n",
    "\n",
    "This programming assignment gives you a chance to perform the classification task using neural networks. You will get to build a neural network from scratch and train and test it on a standard classification dataset. Further you will learn different tricks and techniques to train a neural network eficiently by observing few important issues and trying to overcome them. This includes observing the performance of the network for different activation functions and optimization algorithms. We will conclude with implementation of various regularization techniques to overcome the problems of overfitting and vanishing gradients.\n",
    "\n",
    "<b> Instructions </b>\n",
    "1. Plagiarism is strictly prohibited.\n",
    "2. Delayed submissions will be penalized with a scaling factor of 0.5 per day.\n",
    "3. Please DO NOT use any machine learning libraries unless and otherwise specified.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d89af6",
   "metadata": {},
   "source": [
    "### Basically in Glance\n",
    "\n",
    "- NN Based Classification from Scratch\n",
    "- Understanding various Activation Functions\n",
    "- Understanding Optimization Algorithms\n",
    "- Understanding Regularization Methods\n",
    "- Comparision with Linear Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88b7ff2",
   "metadata": {},
   "source": [
    "#  Part 1 🧠✨ Neural Network Based Digit Classification — MNIST 🖼️🔢\n",
    "\n",
    "This programming assignment focuses on building a **Feedforward Neural Network** from scratch to classify handwritten digits using the **MNIST dataset**. The dataset consists of grayscale images of size **28×28 pixels**, each representing a digit from **0 to 9**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ 1: Load MNIST Data & Create Train-Test Splits\n",
    "\n",
    "📌 **Instructions:**\n",
    "- The MNIST dataset contains **70,000 images** in total.\n",
    "- Split the dataset into:\n",
    "  - ✅ **60,000** images for training  \n",
    "  - 🧪 **10,000** images for testing  \n",
    "- ✅ The code for downloading and splitting the dataset is provided.\n",
    "\n",
    "---\n",
    "\n",
    "## 🏗️  2: Design a Feedforward Classification Network\n",
    "\n",
    "We will build a **3-layer Feedforward Neural Network** with the following architecture:\n",
    "\n",
    "📐 **Network Dimensions:**\n",
    "- Input: 784 (28 × 28 flattened)\n",
    "- Hidden Layer 1: 512 nodes\n",
    "- Hidden Layer 2: 512 nodes\n",
    "- Output Layer: 10 nodes (one for each digit class)\n",
    "\n",
    "🧮 **Mathematical Representation:**\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = h(\\mathbf{W}_3 \\cdot g(\\mathbf{W}_2 \\cdot g(\\mathbf{W}_1 \\cdot \\mathbf{x})))\n",
    "$$\n",
    "\n",
    "- $ \\mathbf{W}_1 \\in \\mathbb{R}^{512 \\times 784} $\n",
    "- $ \\mathbf{W}_2 \\in \\mathbb{R}^{512 \\times 512} $\n",
    "- $ \\mathbf{W}_3 \\in \\mathbb{R}^{10 \\times 512} $\n",
    "\n",
    "✨ **Activations:**\n",
    "- Hidden layers: `ReLU` ⚡  \n",
    "- Output layer: `Softmax` 🎯\n",
    "\n",
    "---\n",
    "\n",
    "## 🏋️  3: Train the Neural Network\n",
    "\n",
    "### 🔄 Steps:\n",
    "1. 📦 **Flatten** 28×28 images into 784-dimensional vectors.  \n",
    "2. 🔁 **Randomly initialize** the weights $ \\mathbf{W}_1, \\mathbf{W}_2, \\mathbf{W}_3 $  \n",
    "3. 🧠 **Feedforward pass** to compute class posteriors.  \n",
    "4. 📉 **Compute loss** between predicted probabilities and true labels (cross-entropy loss suggested).  \n",
    "5. 🔧 **Backpropagate** the loss to compute gradients.  \n",
    "6. ⚙️ **Update parameters** using **Stochastic Gradient Descent (SGD)**.  \n",
    "7. 🧪 **Tune hyperparameters** such as learning rate, batch size, and epochs.\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 4: Evaluate the Model\n",
    "\n",
    "### ✅ Steps:\n",
    "- 🔁 Run a **feedforward pass** on the test data.  \n",
    "- 🧠 **Predict the class** with the highest posterior probability.  \n",
    "- 📉 **Compute loss** and **accuracy**.  \n",
    "- 📋 **Report observations**, performance metrics, and any interesting behaviors.\n",
    "\n",
    "---\n",
    "\n",
    "### 🚀 Bonus Tips:\n",
    "- Try visualizing some predictions 📸  \n",
    "- Experiment with learning rates and see how they affect training!  \n",
    "- Track loss/accuracy over epochs using plots 📈\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642071c6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ade935b3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 🔍🧠 Part 2: Understanding Activation Functions ⚡📉\n",
    "\n",
    "In this part, you will explore how different **activation functions** affect the performance of a feedforward neural network trained on the **MNIST digit classification** task.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 1. Experiment with Activation Functions\n",
    "\n",
    "Train the same network architecture as in Part 1 using **different activation functions** in the hidden layers:\n",
    "\n",
    "🔁 **Activation Functions to try:**\n",
    "- 🌀 `Sigmoid`\n",
    "- 🔄 `Tanh`\n",
    "- ⚡ `ReLU`\n",
    "- ⚡💧 `Leaky ReLU`\n",
    "\n",
    "📌 Keep the following fixed:\n",
    "- Use **Stochastic Gradient Descent (SGD)** as the optimization algorithm\n",
    "- Keep the rest of the architecture, loss function, and learning setup same\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 2. Report & Compare Performance\n",
    "\n",
    "🧠 **Evaluation Instructions:**\n",
    "- Run the trained model on the **MNIST test dataset**\n",
    "- Record and report the **accuracy** for each activation function\n",
    "- ✍️ Write down your **observations** in the report:\n",
    "  - How does the choice of activation affect learning?\n",
    "  - Which function performed best and why?\n",
    "  - Are there any signs of vanishing gradients or training instability?\n",
    "\n",
    "---\n",
    "\n",
    "### 📝 Sample Table for Results (fill this in later):\n",
    "\n",
    "| Activation Function | Test Accuracy (%) | Observations |\n",
    "|---------------------|-------------------|--------------|\n",
    "| Sigmoid             |                   |              |\n",
    "| Tanh                |                   |              |\n",
    "| ReLU                |                   |              |\n",
    "| Leaky ReLU          |                   |              |\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 Tips:\n",
    "- You may visualize training loss curves for each activation function 📉\n",
    "- Consider running multiple trials to average out randomness\n",
    "- Look at confusion matrices to understand misclassification patterns 🔍\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2584c3c7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4565605",
   "metadata": {},
   "source": [
    "# ⚙️📈 Part 3: Understanding Optimization Algorithms 🧠🔧\n",
    "\n",
    "In this part, you'll explore how different **optimization algorithms** affect the training performance of your classification network.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 1. Use the Best Activation Function\n",
    "\n",
    "- Choose the **best-performing activation function** from your experiments in **Part 2** (e.g., ReLU or Tanh).\n",
    "- Use this activation function in your network’s hidden layers.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 2. Train with Adam Optimizer\n",
    "\n",
    "Train the same classification network using the **Adam optimization algorithm**, instead of SGD.\n",
    "\n",
    "🔁 Keep the rest of the setup **unchanged**:\n",
    "- Same architecture (3-layer FFNN)\n",
    "- Same initialization\n",
    "- Same learning rate (or tune slightly if necessary)\n",
    "- Same loss function (Cross-Entropy)\n",
    "\n",
    "---\n",
    "\n",
    "## ⚖️ 3. Compare Performance: SGD vs Adam\n",
    "\n",
    "Evaluate and compare the models trained using:\n",
    "- 🔁 **Stochastic Gradient Descent (SGD)**\n",
    "- ⚙️ **Adam Optimizer**\n",
    "\n",
    "🧪 **Metrics to Compare:**\n",
    "- Test Accuracy\n",
    "- Convergence speed (training loss curves)\n",
    "- Stability of training (variance in accuracy across epochs)\n",
    "\n",
    "---\n",
    "\n",
    "### 📝 Sample Comparison Table:\n",
    "\n",
    "| Optimizer | Activation Function | Test Accuracy (%) | Observations |\n",
    "|-----------|---------------------|-------------------|--------------|\n",
    "| SGD       | ReLU (or best)      |                   |              |\n",
    "| Adam      | ReLU (or best)      |                   |              |\n",
    "\n",
    "---\n",
    "\n",
    "## ✍️ 4. Report Your Observations\n",
    "\n",
    "🔍 **What to observe:**\n",
    "- Did Adam improve the accuracy?\n",
    "- Was convergence faster or slower than SGD?\n",
    "- Any trade-offs noticed (e.g., stability vs. generalization)?\n",
    "\n",
    "🗒️ Document your insights in the report.\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 Tips:\n",
    "- Plot accuracy/loss over epochs for both optimizers\n",
    "- Try using same learning rates for fairness, or optimize them slightly for best results\n",
    "- Optionally try RMSprop, Adagrad, etc., for deeper exploration 🔍\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9021fc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fa95b91",
   "metadata": {},
   "source": [
    "# 🧪🧰 Part 4: Understanding Regularization Methods 🔒📉\n",
    "\n",
    "In this part of the assignment, you'll explore **regularization techniques** to reduce **overfitting** in your neural network. You'll use the network from previous parts and incorporate different methods to improve generalization.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Goal:\n",
    "Retrain the neural network using the following **regularization techniques**, and compare their performance on the **MNIST test set**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔗 1. Weight Regularization (L2 Penalty)\n",
    "\n",
    "- Add an L2 regularization term to the loss function:\n",
    "  \n",
    "  $$\n",
    "  \\text{Loss}_{\\text{total}} = \\text{CrossEntropyLoss} + \\lambda \\left( \\lVert \\mathbf{W}_1 \\rVert^2 + \\lVert \\mathbf{W}_2 \\rVert^2 + \\lVert \\mathbf{W}_3 \\rVert^2 \\right)\n",
    "  $$\n",
    "\n",
    "- Experiment with different values of **$\\lambda$** (e.g., `0.001`, `0.01`, `0.1`) and report the impact.\n",
    "\n",
    "---\n",
    "\n",
    "## 💧 2. Dropout Regularization\n",
    "\n",
    "- Introduce **Dropout** in the hidden layers with a probability of `0.2`.\n",
    "- Dropout randomly disables some neurons during training to prevent co-adaptation.\n",
    "- 🧪 **Important:** Disable Dropout during inference/evaluation.\n",
    "\n",
    "### 🔁 Suggestions:\n",
    "- Try with different probabilities: `0.1`, `0.2`, `0.5`\n",
    "- Compare their effects on validation accuracy\n",
    "\n",
    "---\n",
    "\n",
    "## ⏹️ 3. Early Stopping\n",
    "\n",
    "- Monitor **validation loss** during training\n",
    "- Stop training when validation loss starts increasing, even if training loss is decreasing\n",
    "- Prevents the model from overfitting the training data\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Results Table\n",
    "\n",
    "| Regularization Method | Accuracy on Test Set (%) | Observations |\n",
    "|------------------------|--------------------------|--------------|\n",
    "| L2 (λ = 0.001)         |                          |              |\n",
    "| L2 (λ = 0.01)          |                          |              |\n",
    "| Dropout (p = 0.2)      |                          |              |\n",
    "| Dropout (p = 0.5)      |                          |              |\n",
    "| Early Stopping         |                          |              |\n",
    "\n",
    "---\n",
    "\n",
    "## ✍️ Report Your Observations\n",
    "\n",
    "🧠 Consider reflecting on:\n",
    "- Which regularization method gave the best test accuracy?\n",
    "- Did any method significantly reduce overfitting?\n",
    "- How did training time, convergence, and stability change with each method?\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 Tips:\n",
    "- You can visualize **training vs. validation loss curves** to better understand overfitting and the effect of early stopping.\n",
    "- For Dropout, visualize training accuracy vs test accuracy to observe generalization.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614cabc3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60145f37",
   "metadata": {},
   "source": [
    "# ⚖️📊 Part 5: Comparison with Linear Classifiers 🤖 vs 📉\n",
    "\n",
    "In this final part of the assignment, you'll compare the classification performance of **deep neural networks** and **linear classifiers** on both linearly and non-linearly separable datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 1. Linearly Separable Data Generation\n",
    "\n",
    "📈 **Class 1**:\n",
    "- Gaussian distribution\n",
    "- Mean: $ \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} $\n",
    "- Covariance: $ \\begin{bmatrix} 0.3 & 0.0 \\\\ 0.0 & 0.3 \\end{bmatrix} $\n",
    "\n",
    "📉 **Class 2**:\n",
    "- Gaussian distribution\n",
    "- Mean: $ \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix} $\n",
    "- Covariance: $ \\begin{bmatrix} 0.3 & 0.0 \\\\ 0.0 & 0.3 \\end{bmatrix} $\n",
    "\n",
    "📦 **Split:**\n",
    "- 4500 samples per class for **training**\n",
    "- 500 samples per class for **testing**\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 2. Non-linearly Separable Data\n",
    "\n",
    "- Predefined code provides:\n",
    "  - `class1_data` and `class2_data`\n",
    "  - ~5000 points per class\n",
    "\n",
    "📦 **Split:**\n",
    "- Use **90% for training**, **10% for testing**\n",
    "\n",
    "---\n",
    "\n",
    "# 🧠 Programming Tasks\n",
    "\n",
    "---\n",
    "\n",
    "## 🔸 3. Linear Classification Models - Logistic Regression\n",
    "\n",
    "📘 **Model:**\n",
    "- $ y = \\frac{1}{1 + e^{-\\mathbf{w}^T \\mathbf{x}}} $\n",
    "\n",
    "### ✅ Your tasks:\n",
    "\n",
    "**a) Build a function `Logistic_Regression(X_train, Y_train, X_test)`**:\n",
    "- Initialize $ \\mathbf{w} $ randomly\n",
    "- Optimize using **iterative reweighted least squares**\n",
    "- Predict using learned $ \\mathbf{w} $\n",
    "\n",
    "**b) Evaluate test accuracy**  \n",
    "**c) Visualize decision regions**:\n",
    "- Use boundary plots or color-coded regions\n",
    "\n",
    "---\n",
    "\n",
    "## 🔸 4. Deep Neural Network-Based Classification\n",
    "\n",
    "🏗 **Architecture:**\n",
    "\n",
    "$ \\mathbf{y} = h(\\mathbf{W}_3 \\cdot g(\\mathbf{W}_2 \\cdot g(\\mathbf{W}_1 \\cdot \\mathbf{x}))) $\n",
    "\n",
    "### 🧱 Weight Dimensions:\n",
    "- $ \\mathbf{W}_1 \\in \\mathbb{R}^{3 \\times 2} $\n",
    "- $ \\mathbf{W}_2 \\in \\mathbb{R}^{3 \\times 3} $\n",
    "- $ \\mathbf{W}_3 \\in \\mathbb{R}^{1 \\times 3} $\n",
    "\n",
    "🧩 **Activation Functions**:\n",
    "- $g(.)$ = ReLU (hidden layers)\n",
    "- $h(.)$ = Sigmoid (output layer)\n",
    "\n",
    "🔁 **Posterior Probabilities**:\n",
    "- Class 1: $ \\sigma(z) $\n",
    "- Class 2: $ 1 - \\sigma(z) $\n",
    "\n",
    "### ✅ Your tasks:\n",
    "\n",
    "- Train the network on both datasets\n",
    "- Plot **second layer activation potentials**:\n",
    "  - Input entire data\n",
    "  - Visualize 3D hidden representation\n",
    "- Evaluate performance on test set\n",
    "- Comment on how non-linearity transforms the space\n",
    "\n",
    "---\n",
    "\n",
    "## 🆚 5. Comparison and Observations\n",
    "\n",
    "📊 Compare:\n",
    "- **Logistic Regression (Linear Model)**\n",
    "- **Feedforward Neural Network**\n",
    "\n",
    "### Suggested Comparison Table:\n",
    "\n",
    "| Dataset Type         | Classifier            | Test Accuracy (%) | Comments |\n",
    "|----------------------|------------------------|-------------------|----------|\n",
    "| Linearly Separable   | Logistic Regression    |                   |          |\n",
    "| Linearly Separable   | Neural Network         |                   |          |\n",
    "| Non-linearly Separable | Logistic Regression  |                   |          |\n",
    "| Non-linearly Separable | Neural Network       |                   |          |\n",
    "\n",
    "---\n",
    "\n",
    "## ✍️ Final Observations\n",
    "\n",
    "- Which model performs better on linearly separable data?\n",
    "- Which model excels on non-linear data?\n",
    "- How do hidden activations help in class separation?\n",
    "- Is the added complexity of neural networks justified?\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 Tips:\n",
    "- Use meshgrid and contour plots for decision boundaries\n",
    "- For 3D visualization of hidden layers, use `matplotlib`'s `Axes3D`\n",
    "- Observe the **power of feature transformation** done by neural networks!\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
